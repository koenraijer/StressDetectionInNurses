---
title: "Pilot Data"
author: "Koen Raijer"
date: "2023-01-23"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading library's

```{r}
library("tidyverse")
library("timetk")
library("readxl")
library("zoo")
library("lubridate")
```

## Define helper functions

```{r}
# create_time_series
#
# Input: bs_dir (STRING; base directory of the data), data_src (STRING; HR, EDA, TEMP or ACC), data_loc (STRING; directory where bs_dir is located)
# Output: dataframe with data column and time column (for ACC, the data column contains average body acceleration)

create_time_series <- function(bs_dir, data_src, data_loc) {
  # Load csv
  tmp <- read.csv(paste(data_loc, bs_dir, "/", data_src, ".csv", sep = ""))
  
  if (data_src == "ACC") {
    # Load csv
    # Sampling rate
    sr <- tmp[1,1]
    # Start time
    dt <- as_datetime(suppressWarnings(as.numeric(gsub(pattern = "X", replacement = "", fixed = T, x = names(tmp)))), tz = "Europe/Amsterdam")[1]
    # Trim SR off
    tmp <- tmp[-1,]
    
    # Create time column
    time <- seq(from = dt, by = seconds(1 / sr[[1]]), length.out = nrow(tmp))
    
    tmp <- sqrt(tmp[,1]^2 + tmp[,2]^2 + tmp[,3]^2)
  } else {
    # Sampling rate
    sr <- tmp[1,]
    # Start time
    dt <- as_datetime(as.numeric(gsub(pattern = "X", replacement = "", fixed = T, x = names(tmp))), tz = "Europe/Amsterdam")
    # Trim SR off
    tmp <- tmp[-1,]
    
    # Create time column
    time <- seq(from = dt, by = seconds(1 / sr), length.out = length(tmp))
  }

  # Return dataframe
  df <- data.frame(data_src = tmp, TIME = as_datetime(time, tz = "Europe/Amsterdam"), SRC = bs_dir)
  names(df)[1] <- data_src
  return(df)
}

#--------------------------------------------------------------------------------------------
# add_window_index
#
# Input: data_f (DATA.FRAME), window (in seconds), sr (sample rate in Hz)

add_window_index <- function(data_f, window, sr) {
  # Calc no of time windows
  no_tw <- (nrow(data_f) / sr) / window
  data_f$WINDOW <- rep(1:(no_tw+1), each = window*sr, length.out = nrow(data_f))
  
  # Trim of incomplete time windows
  data_f <- data_f %>% filter(WINDOW <= no_tw) 
  
  return(data_f)
}

#--------------------------------------------------------------------------------------------
# Create helper function that takes baseDir, and returns stats_df for that directory
join_data_from_folders <- function(baseDir) {
    hr_ts <- create_time_series(baseDir, "HR", "input/empatica/")
    eda_ts <- create_time_series(baseDir, "EDA", "input/empatica/")
    temp_ts <- create_time_series(baseDir, "TEMP", "input/empatica/")
    acc_ts <- create_time_series(baseDir, "ACC", "input/empatica/")
    
    # Find max start time and trim all sources to start there
    max_start_time <- max(acc_ts$TIME[1], hr_ts$TIME[1], eda_ts$TIME[1], temp_ts$TIME[1])
    
    hr_ts <- hr_ts %>% filter(TIME >= max_start_time)
    acc_ts <- acc_ts %>% filter(TIME >= max_start_time)
    eda_ts <- eda_ts %>% filter(TIME >= max_start_time)
    temp_ts <- temp_ts %>% filter(TIME >= max_start_time)
    
    # Windowing the time series
    window <- 10 # seconds
    
    hr_ts <- add_window_index(hr_ts, window, 1)
    acc_ts <- add_window_index(acc_ts, window, 32)
    eda_ts <- add_window_index(eda_ts, window, 4)
    temp_ts <- add_window_index(temp_ts, window, 4)
    
    # Find maximum number of windows and trim excess windows
    max_no_windows <- min(max(hr_ts$WINDOW), max(acc_ts$WINDOW), max(eda_ts$WINDOW), max(temp_ts$WINDOW))
    
    hr_ts <- hr_ts %>% filter(WINDOW <= max_no_windows)
    acc_ts <- acc_ts %>% filter(WINDOW <= max_no_windows)
    eda_ts <- eda_ts %>% filter(WINDOW <= max_no_windows)
    temp_ts <- temp_ts %>% filter(WINDOW <= max_no_windows)
    
    # Calculating rolling features per time window (50% overlap)
      # Heart rate
    sr <- 1
    hr_stats <- hr_ts %>% group_by(WINDOW) %>% summarize(
      start_time = min(TIME), 
      end_time = max(TIME), 
      hr_mean = rollapply(HR, width = ((window * sr) / 2), mean, partial = T, by = (window * sr)), 
      hr_sd = rollapply(HR, width = ((window * sr) / 2), sd, partial = T, by = (window * sr)), 
      hr_min = rollapply(HR, width = ((window * sr) / 2), min, partial = T, by = (window * sr)), 
      hr_max = rollapply(HR, width = ((window * sr) / 2), max, partial = T, by = (window * sr)),
    )
    hr_stats <- hr_stats %>% mutate(hr_range = hr_max - hr_min, src = hr_ts$SRC[1])
    
      # Body acceleration
    sr <- 32
    acc_stats <- acc_ts %>% group_by(WINDOW) %>% summarize(
      start_time = min(TIME), 
      end_time = max(TIME), 
      acc_mean = rollapply(ACC, width = ((window * sr) / 2), mean, partial = T, by = (window * sr   )), 
      acc_sd = rollapply(ACC, width = ((window * sr) / 2), sd, partial = T, by = (window * sr)), 
      acc_min = rollapply(ACC, width = ((window * sr) / 2), min, partial = T, by = (window * sr    )), 
      acc_max = rollapply(ACC, width = ((window * sr) / 2), max, partial = T, by = (window * sr    )),
    )
    acc_stats <- acc_stats %>% mutate(acc_range = acc_max - acc_min, src = acc_ts$SRC[1])
    
      # Electrodermal activity
    sr <- 4
    eda_stats <- eda_ts %>% group_by(WINDOW) %>% summarize(
      start_time = min(TIME), 
      end_time = max(TIME), 
      eda_mean = rollapply(EDA, width = ((window * sr) / 2), mean, partial = T, by = (window * sr   )), 
      eda_sd = rollapply(EDA, width = ((window * sr) / 2), sd, partial = T, by = (window * sr)), 
      eda_min = rollapply(EDA, width = ((window * sr) / 2), min, partial = T, by = (window * sr    )), 
      eda_max = rollapply(EDA, width = ((window * sr) / 2), max, partial = T, by = (window * sr    )),
    )
    eda_stats <- eda_stats %>% mutate(eda_range = eda_max - eda_min, src = eda_ts$SRC[1])
    
      # Skin Temperature
    temp_stats <- temp_ts %>% group_by(WINDOW) %>% summarize(
      start_time = min(TIME), 
      end_time = max(TIME), 
      temp_mean = rollapply(TEMP, width = ((window * sr) / 2), mean, partial = T, by = (window * sr   )), 
      temp_sd = rollapply(TEMP, width = ((window * sr) / 2), sd, partial = T, by = (window * sr)), 
      temp_min = rollapply(TEMP, width = ((window * sr) / 2), min, partial = T, by = (window * sr    )), 
      temp_max = rollapply(TEMP, width = ((window * sr) / 2), max, partial = T, by = (window * sr    )),
    )
    temp_stats <- temp_stats %>% mutate(temp_range = temp_max - temp_min, src = temp_ts$SRC[1])
    
    stats_df <- bind_cols(hr_stats[,!(names(hr_stats) %in% c("src"))], acc_stats[,!(names(acc_stats) %in% c("WINDOW", "start_time", "end_time", "src"))], eda_stats[,!(names(eda_stats) %in% c("WINDOW", "start_time", "end_time", "src"))], temp_stats[,!(names(temp_stats) %in% c("WINDOW", "start_time", "end_time"))])
    return(stats_df)
}

#--------------------------------------------------------------------------------------------
# Return labelled df from Inquisit file. 

return_labels_from_file <- function(fileName) {
  inq_df <- read.delim(paste("input/inquisit/", fileName, sep = ""), stringsAsFactors = F, sep = "\t")
  
  inq_df <- inq_df %>% mutate(datetime = as.POSIXct(as.Date(inq_df$date) + hms(inq_df$time), origin = "1970-01-01", tz = "Europe/Amsterdam"))
  
  # Set global options to include milliseconds
  options(digits.secs = 3)          
  
  # Create exact start time column for each trial
    # Init outvec starttime
  starttime <- 1:nrow(inq_df)
  for (i in 1:nrow(inq_df)) {
    # If it's the first item, add the trial times to the datetime
    if (i == 1) {
      starttime[i] <- as_datetime(inq_df$datetime[i], tz = "Europe/Amsterdam")
    # Otherwise, calculate the last row's enddtime to find the current row's starttime:
    } else {
      starttime[i] <- as_datetime(starttime[i-1] + seconds(inq_df$pretrialpause[i - 1] / 1000) + seconds(inq_df$posttrialpause[i - 1] / 1000) + seconds(inq_df$latency[i - 1] / 1000), tz = "Europe/Amsterdam")
    }
  }
  
  inq_df$starttime <- as_datetime(starttime, tz = "Europe/Amsterdam")
  
  # Discard all rows that don't have "TNT" in blockcode column
  inq_df <- inq_df %>% filter(grepl("TNT", inq_df$blockcode, fixed = T))
  # Discard all rows that have "Object" in blockcode column
  inq_df <- inq_df %>% filter(!grepl("Object", inq_df$blockcode, fixed = T))
  # Discard all rows that have "between_TNT" in blockcode column
  inq_df <- inq_df %>% filter(!grepl("between_TNT", inq_df$blockcode, fixed = T))

  # Change response column to make each row of the block have the response 
  response_vec <- vector(mode = "integer", length = nrow(inq_df))
  label_vec <- vector(mode = "character", length = nrow(inq_df))
  
  for (i in unique(inq_df$blocknum)) {
    response <- filter(inq_df, blocknum == i, trialcode == "Intrusion")$response
    type <- filter(inq_df, blocknum == i, trialcode == "Intrusion")$blockcode
    # If it's a Think trial:
    if (grepl("_Think_", type)) {
      # If the answer was 0:
      if(response == 0) {
        label_vec[which(inq_df$blocknum == i)] <- "t_na"
      # If the answer was 2 or 79 (meaning 1/never):
      } else if(response == 2 || response == 79) {
        label_vec[which(inq_df$blocknum == i)] <- "t_wrong"
      # If the answer was 3 or 4 / 80 or 81 (meaning 2/briefly or 3/often):
      } else if(response == 3 || response == 4 || response == 80 || response == 81) {
        label_vec[which(inq_df$blocknum == i)] <- "t"
      }
    # Otherwise, it must be a No-Think trial:
    } else {
      # If the answer was 0:
      if(response == 0) {
        label_vec[which(inq_df$blocknum == i)] <- "nt_na"
      # If the answer was 2 (meaning 1/never):
      } else if(response == 2 || response == 79) {
        label_vec[which(inq_df$blocknum == i)] <- "nt"
      # If the answer was 3 or 4 (meaning 2/briefly or 3/often):
      } else if(response == 3 || response == 4 || response == 80 || response == 81) {
        label_vec[which(inq_df$blocknum == i)] <- "i"
      }
    }
    response_vec[which(inq_df$blocknum == i)] <- response
  }
  
  # Add label and response columns
  inq_df$response <- response_vec
  inq_df$label <- label_vec
  return(inq_df)
}
```

## Data processing

### Empatica

```{r}
# Get list of directories
dir <- paste(getwd(), "/input/empatica/", sep = "")
dir_list <- list.dirs(path = dir, full.names = F, recursive = F)

```

```{r}
# initiate df final_df
features_df <- data.frame()

# Loop over all folders
for (i in 1:length(dir_list)) {
  temp_df <- join_data_from_folders(dir_list[i])
  print((paste("length of temp_df: ", nrow(temp_df))))
  features_df <- bind_rows(features_df, temp_df)
  print((paste("length of features_df: ", nrow(features_df))))
}

# Re-index windows
features <- features_df %>% mutate(WINDOW = row_number())

view(features)
```

### Inquisit

```{r}
# Get list of files
dir <- paste(getwd(), "/input/inquisit/", sep = "")
file_list <- list.files(path = dir, full.names = F, recursive = F)

length(file_list)
```

**Note:** apparently, at some point, the response keys were changed to 79, 80 and 81 (previously 2, 3, and 4). I had some doubts as to the meaning of answer options 79, 80 and 81. I assumed that ascending values would represent never, briefly and often - similar to how 2, 3 and 4 do so. From comparing frequencies of each set of answer options, I have concluded that my first assumption was most likely right.

```{r}
# initiate df final_df
label_df <- data.frame()

# Loop over all folders
k <- length(file_list)
for (i in 1:k) {
  temp_df <- return_labels_from_file(file_list[i])
  print(paste("i =", i))
  print((paste("length of temp_df: ", nrow(temp_df))))
  label_df <- bind_rows(label_df, temp_df)
  print((paste("length of label_df: ", nrow(label_df))))
}
```

**Legend:**

-   `t_na` = think trial without intrusion rating

-   `t_wrong` = unsuccessful think trial, i.e., one where subjects did not manage to think about the scene.

-   `t` = think trial (successful)

-   `nt_na` = no-think trial without intrusion rating

-   `nt` = no-think trial (successful)

-   `i` = intrusion, i.e., unsuccessful no-think trial where subjects thought briefly or often about the scene.

```{r}
# Select only trials where cue is presented
label_df <- label_df %>% filter(grepl("TNT", trialcode))
labels <- label_df %>% select(starttime, label, trialcode)
labels <- labels %>% arrange(starttime, desc = F)

view(labels)
view(features)
# Couple Empatica and Inquisit data 
# - Create interval from row$start_time and row$end_time
# - Check if any label is in 

check_interval <- function(row) {
  # Assign variables for the column indices of "start_time", and "end_time". 
  start_time_col_index <- which(names(features) == "start_time")
  end_time_col_index <- which(names(features) == "end_time")

  # Create interval from row$start_time and row$end_time
  interval <- interval(row[[start_time_col_index]], row[[end_time_col_index]])
  
  # Find indices of labels which occur in the interval
  indices <- which(interval(labels$starttime, labels$starttime + seconds(1), tz = "Europe/Amsterdam") %within% interval(ymd_hms("2022-01-12 13:23:52"), ymd_hms("2022-01-12 13:49:21")))
  print(indices)
  
}

matches <- apply(features, 1, check_interval)

```

```{r}
# From https://lubridate.tidyverse.org/reference/within-interval.html
int <- interval(ymd("2001-01-01"), ymd("2002-01-01"))
int2 <- interval(ymd("2001-06-01"), ymd("2002-01-01"))

ymd("2001-05-03") %within% int # TRUE
#> [1] TRUE
int2 %within% int # TRUE
#> [1] TRUE
ymd("1999-01-01") %within% int # FALSE
#> [1] FALSE
```

I left off at trying to make intervals work. So far, no luck. **Note:** even after paying attention to time zones, there's no overlap.

+---------------------+----------+---------------------+------------+----------------------------+
| Inquisit            |          | Empatica            |            | **Day 2 according to log** |
+=====================+==========+=====================+============+============================+
| ***Date***          | *Rows*   | ***Date***          | ***Rows*** | ***Date***                 |
+---------------------+----------+---------------------+------------+----------------------------+
| 2022-07-01          | 960      |                     |            | 01.07.22                   |
+---------------------+----------+---------------------+------------+----------------------------+
|                     |          |                     |            |                            |
+---------------------+----------+---------------------+------------+----------------------------+
| 2022-07-05          | 960      |                     |            | 05.07.22                   |
+---------------------+----------+---------------------+------------+----------------------------+
| 2022-07-07          | 960      |                     |            | 07.07.22                   |
+---------------------+----------+---------------------+------------+----------------------------+
| 2022-07-09          | 960      |                     |            | 09.07.22                   |
+---------------------+----------+---------------------+------------+----------------------------+
| 2022-07-10          | 960      | 2022-07-11          | 28         | 10.07.22                   |
+---------------------+----------+---------------------+------------+----------------------------+
| **2022-07-12**      | 1920     | **2022-07-12**      | 114        | 12.07.22 /                 |
|                     |          |                     |            |                            |
| 13:14:01 - 13:55:00 |          | 09:34:24 - 09:53:14 |            |                            |
|                     |          |                     |            |                            |
| 16:10:37 - 16:51:30 |          |                     |            |                            |
+---------------------+----------+---------------------+------------+----------------------------+
| **2022-07-13**      | 960      | **2022-07-13**      | 186        | 13.07.22                   |
|                     |          |                     |            |                            |
| 14:28:38 - 14:59:28 |          | 15:10:21 - 15:51:51 |            |                            |
+---------------------+----------+---------------------+------------+----------------------------+
| 2022-07-15          | 1920     | 2022-07-21          | 102        | 15.07.22 /                 |
+---------------------+----------+---------------------+------------+----------------------------+
| 2022-07-22          | 960      | 2022-08-02          | 13         | 22.07.22                   |
+---------------------+----------+---------------------+------------+----------------------------+
| 2022-07-26          | 960      | 2022-08-24          | 73         | 26.07.22                   |
+---------------------+----------+---------------------+------------+----------------------------+

### Test

```{r}
x <- ymd_hms("2022-07-11 07:12:59")
interval_x <- interval(features$start_time[1], features$end_time[1])
print(x)
print(interval_x)

nrow(labels)

Sys.timezone()
```

**Next steps**

-   Find exact moment when cue was presented

-   Couple Empatica and Inquisit data.

## General

What I'm going to do:

-   Divide data up in time windows

-   calculate statistical features for time windows

-   Mark time windows Think (T), No-Think successful (NTs), intrusion

Then, I'm going to make data visualizations to show:

-   Differences in features between T, NTs, NTu windows.

Future improvements:

-   Use `IBI.csv` to add RMSSD and HF power (most useful HRV features).
